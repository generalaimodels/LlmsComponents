We have folders with json files all files with Struture List(dir (str, list(str))) in we have query , retrieve_content , response 

for the given Query search in the  retrieve_content , and previous response, extract mostly relevent content almost 95% match only or above

dic( str , List(str ))  query is mandatory , retrieve_content is all posible content otherwise [],

Helping code snippents:
from tqdm import tqdm
from langchain_huggingface import HuggingFaceEmbeddings

# Model Configuration
MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
MODEL_KWARGS = {'device': 'cpu'}
ENCODE_KWARGS = {'normalize_embeddings': False}
EMBEDDING_MODEL = HuggingFaceEmbeddings(
    model_name=MODEL_NAME,
    model_kwargs=MODEL_KWARGS,
    encode_kwargs=ENCODE_KWARGS
)
from langchain.docstore.document import Document

help snippents
document = Document(
                page_content="Hello, world!",
                metadata={"source": "https://example.com"}
            )

from langchain_community.vectorstores.faiss import FAISS  

nce_scores', 'aadd_documents', 'aadd_texts', 'add_documents', 'add_embeddings', 'add_texts', 'adelete', 
'afrom_documents', 'afrom_embeddings', 'afrom_texts', 'aget_by_ids', 'amax_marginal_relevance_search', 
'amax_marginal_relevance_search_by_vector', 'amax_marginal_relevance_search_with_score_by_vector',
'as_retriever', 'asearch', 'asimilarity_search', 'asimilarity_search_by_vector',
'asimilarity_search_with_relevance_scores', 'asimilarity_search_with_score', 'asimilarity_search_with_score_by_vector',
'astreaming_upsert', 'aupsert', 'delete', 'deserialize_from_bytes', 'embeddings', 'from_documents', 
'from_embeddings', 'from_texts', 'get_by_ids', 'load_local', 'max_marginal_relevance_search', 
'max_marginal_relevance_search_by_vector', 'max_marginal_relevance_search_with_score_by_vector',
'merge_from', 'save_local', 'search', 'serialize_to_bytes', 'similarity_search', 'similarity_search_by_vector', 
'similarity_search_with_relevance_scores', 'similarity_search_with_score', 'similarity_search_with_score_by_vector', 
'streaming_upsert', 'upsert'


from langchain_community.vectorstores.utils import DistanceStrategy
 'COSINE', 'DOT_PRODUCT', 'EUCLIDEAN_DISTANCE', 'JACCARD', 'MAX_INNER_PRODUCT', # give choice to user 


from g4f.client import Client
client = Client()
prompt = f"Query: {query}  {retrieve_content} given response in great details novelity and think about query and generates properly regarding query"
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}]
)
return response.choices[0].message.content

append in json file List(dir)
{
"query": query,
"retrieved_content": retrieved_content,
"response": response
}