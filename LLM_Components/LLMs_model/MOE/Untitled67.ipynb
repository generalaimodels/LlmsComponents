{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ChlMdV45na7z",
        "outputId": "525bc8ab-792b-4623-a553-af578cd3d772"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "trying to initialize the default process group twice!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eb9011b7535e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Initialize process group in the main block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nccl'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or 'gloo' if you are using CPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_AllToAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmsg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_msg_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mfunc_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGroupMember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORLD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trying to initialize the default process group twice!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0mset_pytorch_distributed_envs_from_justknobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: trying to initialize the default process group twice!"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from typing import Any, Callable, Optional, Tuple, Union, cast, TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    Base = nn.Module[Tensor]\n",
        "else:\n",
        "    Base = nn.Module\n",
        "\n",
        "# Initialize process group in the main block\n",
        "if __name__ == \"__main__\":\n",
        "    dist.init_process_group(backend='nccl')  # or 'gloo' if you are using CPUs\n",
        "\n",
        "class _AllToAll(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor) -> Tensor:\n",
        "        ctx.group = group\n",
        "        input = input.contiguous()\n",
        "        output = torch.empty_like(input)\n",
        "        dist.all_to_all_single(output, input, group=group)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor]:\n",
        "        return (None, _AllToAll.apply(ctx.group, *grad_output))\n",
        "\n",
        "class Top2Gate(nn.Module):\n",
        "    def __init__(self, model_dim: int, num_experts: int) -> None:\n",
        "        super().__init__()\n",
        "        self.wg = nn.Linear(model_dim, num_experts, bias=False)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        logits = self.wg(input)\n",
        "        return top2gating(logits)\n",
        "\n",
        "def gumbel_rsample(shape: Tuple[int, ...], device: torch.device) -> Tensor:\n",
        "    gumbel = gumbel_map.get(device)\n",
        "    if gumbel is None:\n",
        "        one = torch.tensor(1.0, device=device)\n",
        "        zero = torch.tensor(0.0, device=device)\n",
        "        gumbel = torch.distributions.gumbel.Gumbel(zero, one).rsample  # type: ignore\n",
        "        gumbel_map[device] = gumbel\n",
        "    return gumbel(shape)\n",
        "\n",
        "def one_hot(tensor: torch.Tensor, num_classes: int) -> Tensor:\n",
        "    assert num_classes > 0, \"num_classes must be a positive integer\"\n",
        "    ret = torch.zeros(tensor.shape + (num_classes,), device=tensor.device, dtype=tensor.dtype)\n",
        "    ret.scatter_(-1, tensor.unsqueeze(-1), 1)\n",
        "    return ret\n",
        "\n",
        "def top2gating(logits: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "    gates = F.softmax(logits, dim=1, dtype=torch.float)\n",
        "    num_tokens = gates.shape[0]\n",
        "    num_experts = gates.shape[1]\n",
        "    capacity = 2 * num_tokens // num_experts\n",
        "    assert num_tokens % num_experts == 0\n",
        "\n",
        "    indices1_s = torch.argmax(gates, dim=1)\n",
        "    mask1 = one_hot(indices1_s, num_experts)\n",
        "\n",
        "    logits_w_noise = logits + gumbel_rsample(logits.shape, device=logits.device)\n",
        "    logits_except1 = logits_w_noise.masked_fill(mask1.bool(), float(\"-inf\"))\n",
        "    indices2_s = torch.argmax(logits_except1, dim=1)\n",
        "    mask2 = one_hot(indices2_s, num_experts)\n",
        "\n",
        "    locations1 = torch.cumsum(mask1, dim=0) - 1\n",
        "    locations2 = torch.cumsum(mask2, dim=0) - 1\n",
        "    locations2 += torch.sum(mask1, dim=0, keepdim=True)\n",
        "\n",
        "    me = torch.mean(gates, dim=0)\n",
        "    ce = torch.mean(mask1.float(), dim=0)\n",
        "    l_aux = torch.mean(me * ce)\n",
        "\n",
        "    mask1 *= torch.lt(locations1, capacity)\n",
        "    mask2 *= torch.lt(locations2, capacity)\n",
        "\n",
        "    locations1_s = torch.sum(locations1 * mask1, dim=1)\n",
        "    locations2_s = torch.sum(locations2 * mask2, dim=1)\n",
        "\n",
        "    gates1_s = (gates * mask1).sum(dim=1)\n",
        "    gates2_s = (gates * mask2).sum(dim=1)\n",
        "    denom_s = gates1_s + gates2_s\n",
        "    denom_s = torch.clamp(denom_s, min=torch.finfo(denom_s.dtype).eps)\n",
        "    gates1_s /= denom_s\n",
        "    gates2_s /= denom_s\n",
        "\n",
        "    gates1 = gates1_s.unsqueeze(-1) * mask1\n",
        "    gates2 = gates2_s.unsqueeze(-1) * mask2\n",
        "    locations1_sc = one_hot(locations1_s, capacity)\n",
        "    locations2_sc = one_hot(locations2_s, capacity)\n",
        "    combine1_sec = gates1.unsqueeze(2) * locations1_sc.unsqueeze(1)\n",
        "    combine2_sec = gates2.unsqueeze(2) * locations2_sc.unsqueeze(1)\n",
        "    combine_weights = combine1_sec + combine2_sec\n",
        "    dispatch_mask = combine_weights.bool()\n",
        "\n",
        "    return l_aux.to(logits.dtype), combine_weights.to(logits.dtype), dispatch_mask\n",
        "\n",
        "class MOELayer(Base):\n",
        "    def __init__(self, gate: nn.Module, experts: Union[nn.Module, nn.ModuleList], group: Optional[Any] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.gate = gate\n",
        "        if isinstance(experts, nn.ModuleList):\n",
        "            self.experts = cast(nn.ModuleList, experts)\n",
        "        else:\n",
        "            self.experts = nn.ModuleList([experts])\n",
        "        self.group = group if group is not None else dist.group.WORLD\n",
        "        for expert in self.experts:\n",
        "            for p in experts.parameters():\n",
        "                p.expert = True  # type: ignore\n",
        "        self.world_size = dist.get_world_size(self.group)\n",
        "        self.num_local_experts = len(self.experts)\n",
        "\n",
        "    def forward(self, *input: Tensor, **kwargs: Any) -> Tensor:\n",
        "        assert len(input) == 1, \"only single input Tensor supported\"\n",
        "        assert len(input[0].shape) == 3, \"input Tensor must have dimensions: (s)equence, (t)oken, (m)odel\"\n",
        "        assert input[0].shape[0] % len(self.experts) == 0, \"num tokens must be order of number of local experts\"\n",
        "\n",
        "        d_model = input[0].shape[2]\n",
        "        reshaped_input = input[0].reshape(-1, d_model)\n",
        "        self.l_aux, combine_weights, dispatch_mask = self.gate(reshaped_input)\n",
        "        dispatched_input = torch.einsum(\"sec,sm->ecm\", dispatch_mask.float(), reshaped_input)\n",
        "        dispatched_input = _AllToAll.apply(self.group, dispatched_input)\n",
        "        dispatched_input = dispatched_input.reshape(self.world_size, self.num_local_experts, -1, d_model)\n",
        "        chunks = dispatched_input.chunk(self.num_local_experts, dim=1)\n",
        "        expert_outputs = [expert(chunk) for chunk, expert in zip(chunks, self.experts)]\n",
        "        expert_output = torch.cat(expert_outputs, dim=1)\n",
        "        expert_output = _AllToAll.apply(self.group, expert_output)\n",
        "        expert_output = expert_output.reshape(self.world_size * self.num_local_experts, -1, d_model)\n",
        "        combined_output = torch.einsum(\"sec,ecm->sm\", combine_weights, expert_output)\n",
        "        return combined_output.reshape(input[0].shape)\n",
        "\n",
        "class GeneralizedDenseMoE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        num_experts: int,\n",
        "        expert_fn: Callable[[int, int], nn.Module]\n",
        "    ) -> None:\n",
        "        super(GeneralizedDenseMoE, self).__init__()\n",
        "        self.gate = Top2Gate(input_dim, num_experts)\n",
        "        self.experts = nn.ModuleList([expert_fn(input_dim, output_dim) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return MOELayer(self.gate, self.experts)(x)\n",
        "\n",
        "# Example usage\n",
        "def example_expert_fn(input_dim: int, output_dim: int) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    num_experts = 4\n",
        "\n",
        "    model = GeneralizedDenseMoE(input_dim, output_dim, num_experts, example_expert_fn)\n",
        "    x = torch.randn(32, input_dim)\n",
        "    output = model(x)\n",
        "    print(output.shape)  # Should be"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from typing import Any, Callable, Optional, Tuple, Union, cast, TYPE_CHECKING,Dict\n",
        "import os\n",
        "\n",
        "# Add this global variable\n",
        "gumbel_map: Dict[torch.device, Callable] = {}\n",
        "if TYPE_CHECKING:\n",
        "    Base = nn.Module[Tensor]\n",
        "else:\n",
        "    Base = nn.Module\n",
        "\n",
        "def initialize_distributed():\n",
        "    # Example setup with manual setting of environment variables for testing\n",
        "    os.environ['RANK'] = '0'\n",
        "    os.environ['WORLD_SIZE'] = '1'\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "    dist.init_process_group(backend='nccl', init_method='env://')  # or 'gloo' for CPUs\n",
        "\n",
        "class _AllToAll(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor) -> Tensor:\n",
        "        ctx.group = group\n",
        "        input = input.contiguous()\n",
        "        output = torch.empty_like(input)\n",
        "        dist.all_to_all_single(output, input, group=group)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor]:\n",
        "        return (None, _AllToAll.apply(ctx.group, *grad_output))\n",
        "\n",
        "class Top2Gate(nn.Module):\n",
        "    def __init__(self, model_dim: int, num_experts: int) -> None:\n",
        "        super().__init__()\n",
        "        self.wg = nn.Linear(model_dim, num_experts, bias=False)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        logits = self.wg(input)\n",
        "        return top2gating(logits)\n",
        "\n",
        "\n",
        "\n",
        "def gumbel_rsample(shape: Tuple[int, ...], device: torch.device) -> Tensor:\n",
        "    gumbel = gumbel_map.get(device)\n",
        "    if gumbel is None:\n",
        "        one = torch.tensor(1.0, device=device)\n",
        "        zero = torch.tensor(0.0, device=device)\n",
        "        gumbel = torch.distributions.gumbel.Gumbel(zero, one).rsample  # type: ignore\n",
        "        gumbel_map[device] = gumbel\n",
        "    return gumbel(shape)\n",
        "\n",
        "def one_hot(tensor: torch.Tensor, num_classes: int) -> Tensor:\n",
        "    assert num_classes > 0, \"num_classes must be a positive integer\"\n",
        "    ret = torch.zeros(tensor.shape + (num_classes,), device=tensor.device, dtype=tensor.dtype)\n",
        "    ret.scatter_(-1, tensor.unsqueeze(-1), 1)\n",
        "    return ret\n",
        "\n",
        "def top2gating(logits: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "    gates = F.softmax(logits, dim=1, dtype=torch.float)\n",
        "    num_tokens = gates.shape[0]\n",
        "    num_experts = gates.shape[1]\n",
        "    capacity = 2 * num_tokens // num_experts\n",
        "    assert num_tokens % num_experts == 0\n",
        "\n",
        "    indices1_s = torch.argmax(gates, dim=1)\n",
        "    mask1 = one_hot(indices1_s, num_experts)\n",
        "\n",
        "    logits_w_noise = logits + gumbel_rsample(logits.shape, device=logits.device)\n",
        "    logits_except1 = logits_w_noise.masked_fill(mask1.bool(), float(\"-inf\"))\n",
        "    indices2_s = torch.argmax(logits_except1, dim=1)\n",
        "    mask2 = one_hot(indices2_s, num_experts)\n",
        "\n",
        "    locations1 = torch.cumsum(mask1, dim=0) - 1\n",
        "    locations2 = torch.cumsum(mask2, dim=0) - 1\n",
        "    locations2 += torch.sum(mask1, dim=0, keepdim=True)\n",
        "\n",
        "    me = torch.mean(gates, dim=0)\n",
        "    ce = torch.mean(mask1.float(), dim=0)\n",
        "    l_aux = torch.mean(me * ce)\n",
        "\n",
        "    mask1 *= torch.lt(locations1, capacity)\n",
        "    mask2 *= torch.lt(locations2, capacity)\n",
        "\n",
        "    locations1_s = torch.sum(locations1 * mask1, dim=1)\n",
        "    locations2_s = torch.sum(locations2 * mask2, dim=1)\n",
        "\n",
        "    gates1_s = (gates * mask1).sum(dim=1)\n",
        "    gates2_s = (gates * mask2).sum(dim=1)\n",
        "    denom_s = gates1_s + gates2_s\n",
        "    denom_s = torch.clamp(denom_s, min=torch.finfo(denom_s.dtype).eps)\n",
        "    gates1_s /= denom_s\n",
        "    gates2_s /= denom_s\n",
        "\n",
        "    gates1 = gates1_s.unsqueeze(-1) * mask1\n",
        "    gates2 = gates2_s.unsqueeze(-1) * mask2\n",
        "    locations1_sc = one_hot(locations1_s, capacity)\n",
        "    locations2_sc = one_hot(locations2_s, capacity)\n",
        "    combine1_sec = gates1.unsqueeze(2) * locations1_sc.unsqueeze(1)\n",
        "    combine2_sec = gates2.unsqueeze(2) * locations2_sc.unsqueeze(1)\n",
        "    combine_weights = combine1_sec + combine2_sec\n",
        "    dispatch_mask = combine_weights.bool()\n",
        "\n",
        "    return l_aux.to(logits.dtype), combine_weights.to(logits.dtype), dispatch_mask\n",
        "\n",
        "class MOELayer(Base):\n",
        "    def __init__(self, gate: nn.Module, experts: Union[nn.Module, nn.ModuleList], group: Optional[Any] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.gate = gate\n",
        "        if isinstance(experts, nn.ModuleList):\n",
        "            self.experts = cast(nn.ModuleList, experts)\n",
        "        else:\n",
        "            self.experts = nn.ModuleList([experts])\n",
        "        self.group = group if group is not None else dist.group.WORLD\n",
        "        for expert in self.experts:\n",
        "            for p in experts.parameters():\n",
        "                p.expert = True  # type: ignore\n",
        "        self.world_size = dist.get_world_size(self.group)\n",
        "        self.num_local_experts = len(self.experts)\n",
        "\n",
        "    def forward(self, *input: Tensor, **kwargs: Any) -> Tensor:\n",
        "        assert len(input) == 1, \"only single input Tensor supported\"\n",
        "        assert len(input[0].shape) == 3, \"input Tensor must have dimensions: (s)equence, (t)oken, (m)odel\"\n",
        "        assert input[0].shape[0] % len(self.experts) == 0, \"num tokens must be order of number of local experts\"\n",
        "\n",
        "        d_model = input[0].shape[2]\n",
        "        reshaped_input = input[0].reshape(-1, d_model)\n",
        "        self.l_aux, combine_weights, dispatch_mask = self.gate(reshaped_input)\n",
        "        dispatched_input = torch.einsum(\"sec,sm->ecm\", dispatch_mask.float(), reshaped_input)\n",
        "        dispatched_input = _AllToAll.apply(self.group, dispatched_input)\n",
        "        dispatched_input = dispatched_input.reshape(self.world_size, self.num_local_experts, -1, d_model)\n",
        "        chunks = dispatched_input.chunk(self.num_local_experts, dim=1)\n",
        "        expert_outputs = [expert(chunk) for chunk, expert in zip(chunks, self.experts)]\n",
        "        expert_output = torch.cat(expert_outputs, dim=1)\n",
        "        expert_output = _AllToAll.apply(self.group, expert_output)\n",
        "        expert_output = expert_output.reshape(self.world_size * self.num_local_experts, -1, d_model)\n",
        "        combined_output = torch.einsum(\"sec,ecm->sm\", combine_weights, expert_output)\n",
        "        return combined_output.reshape(input[0].shape)\n",
        "\n",
        "class GeneralizedDenseMoE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        num_experts: int,\n",
        "        expert_fn: Callable[[int, int], nn.Module]\n",
        "    ) -> None:\n",
        "        super(GeneralizedDenseMoE, self).__init__()\n",
        "        self.gate = Top2Gate(input_dim, num_experts)\n",
        "        self.experts = nn.ModuleList([expert_fn(input_dim, output_dim) for _ in range(num_experts)])\n",
        "        self.moe_layer = MOELayer(self.gate, self.experts)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(0)  # Add sequence dimension\n",
        "        output = self.moe_layer(x)\n",
        "        return output.squeeze(0) if output.size(0) == 1 else output\n",
        "\n",
        "\n",
        "def example_expert_fn(input_dim: int, output_dim: int) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "if __name__ == \"__main__\":\n",
        "    initialize_distributed()\n",
        "\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    num_experts = 4\n",
        "\n",
        "    model = GeneralizedDenseMoE(input_dim, output_dim, num_experts, example_expert_fn)\n",
        "\n",
        "    # Test with 3D input\n",
        "    x_3d = torch.randn(4, 32, input_dim)\n",
        "    output_3d = model(x_3d)\n",
        "    print(\"3D input shape:\", x_3d.shape)\n",
        "    print(\"3D output shape:\", output_3d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YyPJU7WZn7JC",
        "outputId": "e173d2a0-adce-4b97-f735-b72b8cf320aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No backend type associated with device type cpu",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14ddab35ac8f>\u001b[0m in \u001b[0;36m<cell line: 170>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Test with 3D input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mx_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0moutput_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3D input shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3D output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-14ddab35ac8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add sequence dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoe_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-14ddab35ac8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mdispatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sec,sm->ecm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mdispatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_AllToAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatched_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mdispatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatched_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_local_experts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatched_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_local_experts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-14ddab35ac8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, group, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_to_all_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmsg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_msg_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36mall_to_all_single\u001b[0;34m(output, input, output_split_sizes, input_split_sizes, group, async_op)\u001b[0m\n\u001b[1;32m   3515\u001b[0m         )\n\u001b[1;32m   3516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3517\u001b[0;31m         work = group.alltoall_base(\n\u001b[0m\u001b[1;32m   3518\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_split_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3519\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No backend type associated with device type cpu"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from typing import Any, Callable, Optional, Tuple, Union, cast, TYPE_CHECKING,Dict\n",
        "import os\n",
        "\n",
        "# Add this global variable\n",
        "gumbel_map: Dict[torch.device, Callable] = {}\n",
        "if TYPE_CHECKING:\n",
        "    Base = nn.Module[Tensor]\n",
        "else:\n",
        "    Base = nn.Module\n",
        "def initialize_distributed(backend='gloo'):\n",
        "    # Example setup with manual setting of environment variables for testing\n",
        "    os.environ['RANK'] = '0'\n",
        "    os.environ['WORLD_SIZE'] = '1'\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "    dist.init_process_group(backend=backend, init_method='env://')\n",
        "\n",
        "\n",
        "class _AllToAll(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor) -> Tensor:\n",
        "        ctx.group = group\n",
        "        input = input.contiguous()\n",
        "        output = torch.empty_like(input)\n",
        "        dist.all_to_all_single(output, input, group=group)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor]:\n",
        "        return (None, _AllToAll.apply(ctx.group, *grad_output))\n",
        "\n",
        "\n",
        "class Top2Gate(nn.Module):\n",
        "    def __init__(self, model_dim: int, num_experts: int) -> None:\n",
        "        super().__init__()\n",
        "        self.wg = nn.Linear(model_dim, num_experts, bias=False)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        logits = self.wg(input)\n",
        "        return top2gating(logits)\n",
        "\n",
        "\n",
        "def gumbel_rsample(shape: Tuple[int, ...], device: torch.device) -> Tensor:\n",
        "    gumbel = gumbel_map.get(device)\n",
        "    if gumbel is None:\n",
        "        one = torch.tensor(1.0, device=device)\n",
        "        zero = torch.tensor(0.0, device=device)\n",
        "        gumbel = torch.distributions.gumbel.Gumbel(zero, one).rsample  # type: ignore\n",
        "        gumbel_map[device] = gumbel\n",
        "    return gumbel(shape)\n",
        "\n",
        "\n",
        "def one_hot(tensor: torch.Tensor, num_classes: int) -> Tensor:\n",
        "    assert num_classes > 0, \"num_classes must be a positive integer\"\n",
        "    ret = torch.zeros(tensor.shape + (num_classes,), device=tensor.device, dtype=tensor.dtype)\n",
        "    ret.scatter_(-1, tensor.unsqueeze(-1), 1)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def top2gating(logits: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "    gates = F.softmax(logits, dim=1, dtype=torch.float)\n",
        "    num_tokens = gates.shape[0]\n",
        "    num_experts = gates.shape[1]\n",
        "    capacity = 2 * num_tokens // num_experts\n",
        "    assert num_tokens % num_experts == 0\n",
        "\n",
        "    indices1_s = torch.argmax(gates, dim=1)\n",
        "    mask1 = one_hot(indices1_s, num_experts)\n",
        "\n",
        "    logits_w_noise = logits + gumbel_rsample(logits.shape, device=logits.device)\n",
        "    logits_except1 = logits_w_noise.masked_fill(mask1.bool(), float(\"-inf\"))\n",
        "    indices2_s = torch.argmax(logits_except1, dim=1)\n",
        "    mask2 = one_hot(indices2_s, num_experts)\n",
        "\n",
        "    locations1 = torch.cumsum(mask1, dim=0) - 1\n",
        "    locations2 = torch.cumsum(mask2, dim=0) - 1\n",
        "    locations2 += torch.sum(mask1, dim=0, keepdim=True)\n",
        "\n",
        "    me = torch.mean(gates, dim=0)\n",
        "    ce = torch.mean(mask1.float(), dim=0)\n",
        "    l_aux = torch.mean(me * ce)\n",
        "\n",
        "    mask1 *= torch.lt(locations1, capacity)\n",
        "    mask2 *= torch.lt(locations2, capacity)\n",
        "\n",
        "    locations1_s = torch.sum(locations1 * mask1, dim=1)\n",
        "    locations2_s = torch.sum(locations2 * mask2, dim=1)\n",
        "\n",
        "    gates1_s = (gates * mask1).sum(dim=1)\n",
        "    gates2_s = (gates * mask2).sum(dim=1)\n",
        "    denom_s = gates1_s + gates2_s\n",
        "    denom_s = torch.clamp(denom_s, min=torch.finfo(denom_s.dtype).eps)\n",
        "    gates1_s /= denom_s\n",
        "    gates2_s /= denom_s\n",
        "\n",
        "    gates1 = gates1_s.unsqueeze(-1) * mask1\n",
        "    gates2 = gates2_s.unsqueeze(-1) * mask2\n",
        "    locations1_sc = one_hot(locations1_s, capacity)\n",
        "    locations2_sc = one_hot(locations2_s, capacity)\n",
        "    combine1_sec = gates1.unsqueeze(2) * locations1_sc.unsqueeze(1)\n",
        "    combine2_sec = gates2.unsqueeze(2) * locations2_sc.unsqueeze(1)\n",
        "    combine_weights = combine1_sec + combine2_sec\n",
        "    dispatch_mask = combine_weights.bool()\n",
        "\n",
        "    return l_aux.to(logits.dtype), combine_weights.to(logits.dtype), dispatch_mask\n",
        "\n",
        "\n",
        "class MOELayer(Base):\n",
        "    def __init__(self, gate: nn.Module, experts: Union[nn.Module, nn.ModuleList], group: Optional[Any] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.gate = gate\n",
        "        if isinstance(experts, nn.ModuleList):\n",
        "            self.experts = cast(nn.ModuleList, experts)\n",
        "        else:\n",
        "            self.experts = nn.ModuleList([experts])\n",
        "        self.group = group if group is not None else dist.group.WORLD\n",
        "        for expert in self.experts:\n",
        "            for p in experts.parameters():\n",
        "                p.expert = True  # type: ignore\n",
        "        self.world_size = dist.get_world_size(self.group)\n",
        "        self.num_local_experts = len(self.experts)\n",
        "\n",
        "    def forward(self, *input: Tensor, **kwargs: Any) -> Tensor:\n",
        "        assert len(input) == 1, \"only single input Tensor supported\"\n",
        "        assert len(input[0].shape) == 3, \"input Tensor must have dimensions: (s)equence, (t)oken, (m)odel\"\n",
        "        assert input[0].shape[0] % len(self.experts) == 0, \"num tokens must be order of number of local experts\"\n",
        "\n",
        "        d_model = input[0].shape[2]\n",
        "        reshaped_input = input[0].reshape(-1, d_model)\n",
        "        self.l_aux, combine_weights, dispatch_mask = self.gate(reshaped_input)\n",
        "        dispatched_input = torch.einsum(\"sec,sm->ecm\", dispatch_mask.float(), reshaped_input)\n",
        "        dispatched_input = _AllToAll.apply(self.group, dispatched_input)\n",
        "        dispatched_input = dispatched_input.reshape(self.world_size, self.num_local_experts, -1, d_model)\n",
        "        chunks = dispatched_input.chunk(self.num_local_experts, dim=1)\n",
        "        expert_outputs = [expert(chunk) for chunk, expert in zip(chunks, self.experts)]\n",
        "        expert_output = torch.cat(expert_outputs, dim=1)\n",
        "        expert_output = _AllToAll.apply(self.group, expert_output)\n",
        "        expert_output = expert_output.reshape(self.world_size * self.num_local_experts, -1, d_model)\n",
        "        combined_output = torch.einsum(\"sec,ecm->sm\", combine_weights, expert_output)\n",
        "        return combined_output.reshape(input[0].shape)\n",
        "\n",
        "\n",
        "class GeneralizedDenseMoE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        num_experts: int,\n",
        "        expert_fn: Callable[[int, int], nn.Module]\n",
        "    ) -> None:\n",
        "        super(GeneralizedDenseMoE, self).__init__()\n",
        "        self.gate = Top2Gate(input_dim, num_experts)\n",
        "        self.experts = nn.ModuleList([expert_fn(input_dim, output_dim) for _ in range(num_experts)])\n",
        "        self.moe_layer = MOELayer(self.gate, self.experts)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(0)  # Add sequence dimension\n",
        "        output = self.moe_layer(x)\n",
        "        return output.squeeze(0) if output.size(0) == 1 else output\n",
        "\n",
        "\n",
        "def example_expert_fn(input_dim: int, output_dim: int) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    initialize_distributed(backend=\"nccl\" if torch.cuda.is_available() else \"gloo\")\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    num_experts = 4\n",
        "\n",
        "    model = GeneralizedDenseMoE(input_dim, output_dim, num_experts, example_expert_fn)\n",
        "\n",
        "    # Test with 3D input\n",
        "    x_3d = torch.randn(4, 32, input_dim)\n",
        "    output_3d = model(x_3d)\n",
        "    print(\"3D input shape:\", x_3d.shape)\n",
        "    print(\"3D output shape:\", output_3d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0bvZb6aso2K2",
        "outputId": "005037b6-5226-4280-fe60-dcae4359c2c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "trying to initialize the default process group twice!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-32c93b6279ea>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0minitialize_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nccl\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"gloo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-32c93b6279ea>\u001b[0m in \u001b[0;36minitialize_distributed\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MASTER_PORT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'12345'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'env://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmsg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_msg_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mfunc_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGroupMember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORLD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trying to initialize the default process group twice!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0mset_pytorch_distributed_envs_from_justknobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: trying to initialize the default process group twice!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-QduVVQrDT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}